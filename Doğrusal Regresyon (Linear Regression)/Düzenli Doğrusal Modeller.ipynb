{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aşırı öğrenmeyi(overfitting) azaltmanın bit yolu modeli düzenlileştirmektir(yani modeli kısıtlamaktır.). Örneğin bir polinom modeli kısıtlamanın basit bir yolu polinom derecelerinin sayısını azaltmaktır.\n\nDoğrusal bir model için düzenlileştirme,genellikle modelin katsayılarını(ağırlıklarını - weights) kısıtlayarak gerçekleştirilir. Modelin katsayılarını kısıtlamak için 3 farklı yol uygulayan Ridge Regression, Lasso Regression, ve Elastic Net'e bakacağız:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Ridge Regresyon\n\nRidge Regresyon(Tikhonov regularization da denir), Doğrusal Regresyonun düzenlileştirilmiş bir versiyonudur: $\\alpha \\sum_{i=1}^{n} \\theta_{i}^{2}$ denklemine eşit olan düzenlileştirme terimi, maliyet fonksiyonuna eklenir. Bu,öğrenme algoritmasını hem verilere uymaya ,hemde modelin ağırlıklarını mümkün olduğunca küçük tutmaya zorlar. Dikkat edilmesi gereken bir nokta: düzenlileştirme terimi,sadece eğitim sırasında maliyet fonksiyonuna eklenmelidir. Modeli eğittikten sonra,modeli değerlendirmek için düzenleştirilmemiş performans ölçütleri kullanabilirsiniz.\n\n$\\alpha$ hiperparametresi,modeli ne kadar düzenlileştirmek istediğinizi kontrol eder. Eğer $\\alpha=0$ ise, Ridge Regresyon bir Doğrusal Regresyon olur. Eğer $\\theta$ çok büyük ise tüm ağırlıklar sıfıra çok yakın olacaktır.\nRidge Regresyon maliyet fonksiyonu:\n\n$$RSS_{RIDGE} = \\sum_{i=1}^{m}(h_{\\theta}(x_{i})-y_{i})^{2} + \\alpha \\sum_{j=1}^{n}\\theta^{2}_{j}$$\n\nBias terimi $\\theta_{0}$'ın düzenlileştirilmediğine dikkat edin(toplama i=1'den başlıyor.).\n\n---------------\n\n# Uyarı\n\nGirdi özelliklerinin ölçeğine duyarlı olduğu için Ridge Regresyon uygulamadan önce verileri ölçeklendirmek (örneğin,StandartScaller kullanarak) önemlidir. Bu,çoğu doğrusal model için geçerlidir.\n\n---------------\n\nDoğrusal Regresyon'da olduğu gibi,Ridge Regresyonu uygulamak için kapalı-form bir denklem(doğrusal regresyondaki Normal Denklem gibi) veya Gradient Descent kullanabiliriz.\n\nRidge Regresyon kapalı-form çözümü:\n\n$$\\hat{\\theta} = (X^{T}X+\\alpha A)^{-1}X^{T}y$$\n\nNOT: Denklemdeki A:(n+1)x(n+1)'lik birim matristir.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scikit-Learn kullanarak Ridge Regresyon uygulaması:\n\n# Önce rasggele veriler oluşturalım:\nimport numpy as np\nimport numpy.random as rnd\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\nm = 100\nX = 6 * np.random.rand(m, 1) - 3\ny = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n\n# Ridge Regresyon (kapalı-form çözüm kullanarak)\nfrom sklearn.linear_model import Ridge\nridge_reg = Ridge(alpha=1, solver=\"cholesky\")\nridge_reg.fit(X, y)\nridge_reg.predict([[1.5]])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Ridge Regresyon (Stochastic Gradient Descent kullanarak):\n\nfrom sklearn.linear_model import SGDRegressor\nsgd_reg_ridge = SGDRegressor(penalty=\"l2\")\nsgd_reg_ridge.fit(X, y.ravel())\nsgd_reg_ridge.predict([[1.5]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"penalty hiperparametresi kullanılacak düzenlileştirme teriminin tipini belirler. \"l2\" olarak belirtirsek,maliyet fonksiyonuna model ağırlıklarının karesi eklenir.Buna \"l2\" norm da denir ve maliyet fonksiyonuna model ağırlıklarını l2 normunun eklenmesi basitçe Ridge Regresyondur.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Lasso Regresyon\n\nLeast Absolute Shrinkage and Selection Operator Regression(kısaca Lasso Regression), Doğrusal Regresyonun düzenlileştirilmiş başka bir versiyonudur: Ridge Regresyon'da olduğu gibi, maliyet fonksiyonuna bir düzenlileştirme terimi eklenir. Ancak Lasso Regresyon ağırlık vektörünün \"l1\" normunu kullanır.\n\nLasso Regresyon Maliyet Fonksiyonu:\n\n$$RSS_{LASSO} = \\sum_{i=1}^{m}(h_{\\theta}(x_{i})-y_{i})^{2} + \\alpha \\sum_{j=1}^{n}|\\theta_{j}|$$\n\nLasso Regresyonun önemli bir özelliği,en az öneme sahip özellikleri sıfıra eşitleyerek onları eleme eğiliminde olmasıdır.Başka bir deyişle, Lasso Regresyon otomatik olarak özellik seçimi gerçekleştirir.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lasso sınıfını kullanan küçük bir Scikit-Learn örneği:\n\nfrom sklearn.linear_model import Lasso\nlasso_reg = Lasso(alpha=0.1)\nlasso_reg.fit(X,y)\nlasso_reg.predict([[1.5]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lasso Regresyon (Stochastic Gradient Descent kullanarak):\n\nfrom sklearn.linear_model import SGDRegressor\nsgd_reg_lasso = SGDRegressor(penalty=\"l1\")\nsgd_reg_lasso.fit(X, y.ravel())\nsgd_reg_lasso.predict([[1.5]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Elastic Net\n\nElastic Net, Ridge Regresyon ile Lasso Regresyonun ortasındadır. Elastic Net'in maliyet fonksiyonu Ridge Regresyonun maliyet fonksiyonu ile Lasso Regresyonun maliyet fonksiyonunun karışımıdır ve bu karışımı,r hiperparametresini kullanarak kontrol edebilirsiniz. r=0 olduğunda Elastic Net, Ridge Regresyona denktir ve r=1 olduğunda Elastic Net, Lasso Regresyona denktir.\n\nElastic Net Maliyet Fonksiyonu:\n\n$$J(\\theta)=MSE(\\theta)+r \\alpha \\sum_{i=1}^n |\\theta_{i}|+\\frac{1-r}{2} \\alpha \\sum_{i=1}^{n} \\theta_{i}^{2}$$","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scikit-Learn’ün Elastik Net sınıfını kullanan kısa bir örnek (l1_ratio, r karışım oranına karşılık gelir):\n\nfrom sklearn.linear_model import ElasticNet\nelastic_net=ElasticNet(l1_ratio=0.5,alpha=0.1)\nelastic_net.fit(X,y)\nelastic_net.predict([[1.5]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Peki ne zaman düz Doğrusal Regresyon(yani herhangi bir düzenlileştirme olmadan),Ridge,Lasso veya Elastic Net kullanmalısınız? En azından biraz düzenlileştirme olması neredeyse her zaman tercih edilir bu sebeple düz Doğrusal Regresyondan kaçınmalısınız. Ridge iyi bir varsayılandır ancak eğer sadece birkaç özelliğin faydalı,işe yarar olduğunu düşünüyorsanız Lasso veya Elastic Net tercih etmelisiniz çünkü Lasso ve Elastic Net faydasız,işe yaramaz özelliklerin ağırlıklarını sıfır yapma eğilimindedirler. Genel olarak Lasso'ya karşı Elastic Net tercih edilir çünkü Lasso, özellik sayısı eğitim örneği sayısından daha fazla olduğunda veya birkaç özellik güçlü bir şekilde ilişkili(correlated) olduğunda düzensiz davranabilir.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}